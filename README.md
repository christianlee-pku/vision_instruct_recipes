# ğŸŒ‹ Vision Instruct Recipes

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![Hydra](https://img.shields.io/badge/Config-Hydra-89b8cd)](https://hydra.cc/)
[![Code Style](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)

A production-ready pipeline for training **LLaVA (Large Language and Vision Assistant)** models, designed to seamlessly scale from **local consumer GPUs** to **cloud clusters**.

## ğŸ“° News

- **[2026-01-21]** ğŸ“˜ **Documentation Expansion**: Added comprehensive troubleshooting guides, API references, and developer contribution guidelines.
- **[2026-01-20]** ğŸš€ **v0.2.0 Release**: 
    - **Refined Output Structure**: Training outputs are now organized by experiment, model, and hyperparameters (e.g., `outputs/cloud_gpu_scale/openai_clip...`) for easier tracking.
    - **Enhanced Cloud Script**: `scripts/train_cloud.sh` now supports optional arguments for experiment names and custom GPU counts (e.g., `bash scripts/train_cloud.sh cloud_gpu_scale 4`).
    - **Documentation Update**: Comprehensive guides for CPU/GPU training and getting started have been consolidated and improved.
- **[2026-01-19]** ğŸ”¥ Initial release of Vision Instruct Recipes!

## âœ¨ Features

- **ğŸš€ Scalable Training**: Seamlessly switch between local debugging (single GPU) and cloud-scale training (multi-GPU with DeepSpeed ZeRO-2).
- **ğŸ’¡ Efficient Fine-Tuning**: Built-in support for **4-bit QLoRA** and **Gradient Checkpointing**, enabling training on consumer cards (e.g., RTX 3090/4090).
- **ğŸ› ï¸ Modular Design**: Clean separation of model architecture, data loading, and training logic using Hydra configuration.
- **ğŸ“Š Experiment Tracking**: Integrated with **WandB** for real-time loss tracking and visual generation callbacks.

## ğŸ—ï¸ Pipeline Overview

```mermaid
graph TD
    A[Configuration - Hydra] --> B[Model Loading];
    B -->|LLaVA Arch| C[Base LLM - Llama-3];
    B -->|Vision Mixin| D[Vision Tower - CLIP];
    B -->|Adapter| E[Projector - MLP];
    
    F[Data Processing] -->|Lazy Loading| G[Tokenization & Masking];
    G --> H[Training Loop];
    
    C & D & E --> H;
    
    H -->|Forward Pass| I[Multimodal Fusion];
    H -->|Backward Pass| J[Gradient Updates - QLoRA];
    
    J --> K[Checkpointing]
    K --> L[WandB Logging]
```

## ğŸ“š Documentation

For detailed guides, please refer to the `docs/` directory:

- **[Getting Started](docs/getting_started.md)**: Comprehensive guide for setting up and running the pipeline.
- **[Technical Architecture](docs/technical_architecture.md)**: Deep dive into the model, data, and training logic.
- **[Training on GPU](docs/train_gpu.md)**: Best practices for cloud-scale and local GPU training.
- **[Training on CPU](docs/train_cpu.md)**: Guide for debugging and testing on CPU.
- **[Inference & Demo](docs/inference.md)**: How to run the Gradio demo and inference scripts.
- **[API Reference](docs/api_reference.md)**: High-level overview of modules and classes.
- **[Troubleshooting](docs/troubleshooting.md)**: Solutions for common errors.

## ğŸ› ï¸ Installation

> ğŸ’¡ For a comprehensive setup guide, see **[Getting Started](docs/getting_started.md)**.

1. **Clone the repository**
   ```bash
   git clone https://github.com/christianlee-pku/vision_instruct_recipes.git
   cd vision_instruct_recipes
   ```

2. **Create a Conda environment**
   ```bash
   conda create -n llava python=3.10
   conda activate llava
   ```

3. **Install dependencies**
   
   - **For Cloud/GPU Users (Recommended)**:
     Optimized for CUDA-enabled GPUs and QLoRA stability.
     **Note**: Ensure your installed CUDA version matches the requirements of the packages (e.g., PyTorch 2.1+).
     ```bash
     # Install GPU-optimized stack
     pip install -r requirements_gpu.txt
     ```

   - **For Local/CPU Users (Debugging)**:
     Lightweight setup for code verification on laptops.
     ```bash
     pip install -r requirements_cpu.txt
     ```

## ğŸš€ Usage

### 1. Data Preparation
1. Download the COCO 2017 dataset images to `data/coco/images`.
2. Place your instruction tuning JSON file (e.g., `llava_instruct_150k.json`) in `data/coco/`.
3. Update `configs/data/default.yaml` if your paths differ.

### 2. Local Debugging (Lite Mode)
> ğŸ“˜ See **[CPU Training Guide](docs/train_cpu.md)** or **[GPU Training Guide](docs/train_gpu.md)** for detailed instructions.

Run a quick training loop on a single GPU (or CPU) to verify the pipeline. This profile uses QLoRA and aggressive memory optimization.

```bash
# Run using the provided script
bash scripts/train_local.sh

# Or directly with python
python scripts/train.py experiment=local_lite
```

### 3. Cloud Scale Training
> ğŸš€ See **[GPU Training Guide](docs/train_gpu.md)** for multi-node/DeepSpeed configurations.

Run full-scale distributed training using DeepSpeed ZeRO-2.

```bash
# Run using the provided script
bash scripts/train_cloud.sh

# Or using accelerate directly
accelerate launch scripts/train.py experiment=cloud_scale
```

### 4. Interactive Demo
> ğŸ¤– See **[Inference & Demo Guide](docs/inference.md)** for details on using the Gradio UI.

Launch a Gradio web interface to chat with your trained model.

```bash
python scripts/demo.py \
    --base_model HuggingFaceTB/SmolLM-135M \
    --adapter ./checkpoints/local_lite/checkpoint-final
```

## ğŸ“‚ Project Structure

```text
vision_instruct_recipes/
â”œâ”€â”€ checkpoints/         # Model artifacts and training checkpoints
â”œâ”€â”€ configs/             # Hydra configuration files
â”‚   â”œâ”€â”€ data/            # Dataset configurations
â”‚   â”œâ”€â”€ deepspeed/       # DeepSpeed optimization configs
â”‚   â”œâ”€â”€ experiment/      # Experiment profiles (local_lite, cloud_scale, etc.)
â”‚   â”œâ”€â”€ hydra/           # Hydra core settings
â”‚   â”œâ”€â”€ model/           # Model architecture configs
â”‚   â””â”€â”€ training/        # Training hyperparameters
â”œâ”€â”€ data/                # Dataset storage (e.g., COCO images, JSONs)
â”œâ”€â”€ docs/                # Project documentation and guides
â”œâ”€â”€ outputs/             # Hydra output directories (logs per run)
â”œâ”€â”€ scripts/             # Execution and utility scripts
â”‚   â”œâ”€â”€ demo.py          # Interactive Gradio demo
â”‚   â””â”€â”€ train.py         # Main training entry point
â”œâ”€â”€ specs/               # Feature specifications and planning documents
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ data/            # Data loading, entities, and collators
â”‚   â”œâ”€â”€ models/          # LLaVA architecture, encoder, and projector
â”‚   â”œâ”€â”€ training/        # Trainer loop and callbacks
â”‚   â””â”€â”€ utils/           # Logging and configuration helpers
â”œâ”€â”€ tests/               # Unit and integration tests
â””â”€â”€ wandb/               # Weights & Biases experiment logs
```

## ğŸ“œ License

This project is licensed under the Apache 2.0 License.

## ğŸ–Šï¸ Citation

If you use this codebase in your research, please cite:

```bibtex
@misc{vision_instruct_recipes,
  author = {Christian Lee},
  title = {Vision Instruct Recipes: Scalable LLaVA Training Pipeline},
  year = {2026},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/christianlee-pku/vision_instruct_recipes}}
}
```
